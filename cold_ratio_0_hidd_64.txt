nohup: ignoring input
Training log will be saved to: runs/20250508_001635_team/training_log.log
Run directory: runs/20250508_001635_team
Train years: [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]
Test years:  [2016, 2017, 2018, 2019]
Device: cuda:3
Current run arguments saved to runs/20250508_001635_team/current_run_args.txt
Starting training from epoch 1 to 800.
[00:17:37] Epoch 001  Loss: 2.9410  L_pred:2.9204  L_time:0.0412    train.py:203
           Loss:2.9410                                                          
[00:18:39] Epoch 002  Loss: 2.9300  L_pred:2.9114  L_time:0.0371    train.py:203
           Loss:2.9300                                                          
[00:19:43] Epoch 003  Loss: 2.9202  L_pred:2.9033  L_time:0.0339    train.py:203
           Loss:2.9202                                                          
[00:20:46] Epoch 004  Loss: 2.9112  L_pred:2.8956  L_time:0.0313    train.py:203
           Loss:2.9112                                                          
[00:21:50] Epoch 005  Loss: 2.9033  L_pred:2.8885  L_time:0.0295    train.py:203
           Loss:2.9033                                                          
[00:22:54] Epoch 006  Loss: 2.8971  L_pred:2.8829  L_time:0.0284    train.py:203
           Loss:2.8971                                                          
[00:23:58] Epoch 007  Loss: 2.8915  L_pred:2.8779  L_time:0.0272    train.py:203
           Loss:2.8915                                                          
[00:25:02] Epoch 008  Loss: 2.8865  L_pred:2.8734  L_time:0.0263    train.py:203
           Loss:2.8865                                                          
[00:26:06] Epoch 009  Loss: 2.8823  L_pred:2.8695  L_time:0.0255    train.py:203
           Loss:2.8823                                                          
[00:27:10] Epoch 010  Loss: 2.8781  L_pred:2.8657  L_time:0.0247    train.py:203
           Loss:2.8781                                                          
[00:28:13] Epoch 011  Loss: 2.8739  L_pred:2.8620  L_time:0.0239    train.py:203
           Loss:2.8739                                                          
[00:29:17] Epoch 012  Loss: 2.8698  L_pred:2.8582  L_time:0.0232    train.py:203
           Loss:2.8698                                                          
[00:30:21] Epoch 013  Loss: 2.8660  L_pred:2.8548  L_time:0.0224    train.py:203
           Loss:2.8660                                                          
[00:31:25] Epoch 014  Loss: 2.8624  L_pred:2.8515  L_time:0.0218    train.py:203
           Loss:2.8624                                                          
[00:32:29] Epoch 015  Loss: 2.8587  L_pred:2.8481  L_time:0.0213    train.py:203
           Loss:2.8587                                                          
[00:33:33] Epoch 016  Loss: 2.8551  L_pred:2.8447  L_time:0.0208    train.py:203
           Loss:2.8551                                                          
[00:34:37] Epoch 017  Loss: 2.8514  L_pred:2.8411  L_time:0.0206    train.py:203
           Loss:2.8514                                                          
[00:35:42] Epoch 018  Loss: 2.8477  L_pred:2.8373  L_time:0.0208    train.py:203
           Loss:2.8477                                                          
[00:36:46] Epoch 019  Loss: 2.8440  L_pred:2.8334  L_time:0.0212    train.py:203
           Loss:2.8440                                                          
[00:37:50] Epoch 020  Loss: 2.8402  L_pred:2.8293  L_time:0.0217    train.py:203
           Loss:2.8402                                                          
[00:38:54] Epoch 021  Loss: 2.8365  L_pred:2.8252  L_time:0.0225    train.py:203
           Loss:2.8365                                                          
[00:39:58] Epoch 022  Loss: 2.8328  L_pred:2.8212  L_time:0.0231    train.py:203
           Loss:2.8328                                                          
[00:41:03] Epoch 023  Loss: 2.8293  L_pred:2.8175  L_time:0.0236    train.py:203
           Loss:2.8293                                                          
[00:42:07] Epoch 024  Loss: 2.8261  L_pred:2.8143  L_time:0.0236    train.py:203
           Loss:2.8261                                                          
[00:43:11] Epoch 025  Loss: 2.8231  L_pred:2.8117  L_time:0.0227    train.py:203
           Loss:2.8231                                                          
Eval Epoch 025 (team) MALE [0.45101693272590637, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5573427677154541, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch025_male0_0.4510_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch025_male0.4510_team.pt (MALE: 0.4510 
at epoch 25)
[00:49:50] Epoch 026  Loss: 2.8203  L_pred:2.8097  L_time:0.0213    train.py:203
           Loss:2.8203                                                          
[00:50:54] Epoch 027  Loss: 2.8178  L_pred:2.8081  L_time:0.0193    train.py:203
           Loss:2.8178                                                          
[00:51:58] Epoch 028  Loss: 2.8155  L_pred:2.8069  L_time:0.0172    train.py:203
           Loss:2.8155                                                          
[00:53:02] Epoch 029  Loss: 2.8135  L_pred:2.8059  L_time:0.0151    train.py:203
           Loss:2.8135                                                          
[00:54:06] Epoch 030  Loss: 2.8117  L_pred:2.8051  L_time:0.0132    train.py:203
           Loss:2.8117                                                          
[00:55:10] Epoch 031  Loss: 2.8101  L_pred:2.8043  L_time:0.0117    train.py:203
           Loss:2.8101                                                          
[00:56:14] Epoch 032  Loss: 2.8088  L_pred:2.8036  L_time:0.0105    train.py:203
           Loss:2.8088                                                          
[00:57:18] Epoch 033  Loss: 2.8076  L_pred:2.8029  L_time:0.0095    train.py:203
           Loss:2.8076                                                          
[00:58:22] Epoch 034  Loss: 2.8066  L_pred:2.8022  L_time:0.0089    train.py:203
           Loss:2.8066                                                          
[00:59:26] Epoch 035  Loss: 2.8057  L_pred:2.8015  L_time:0.0083    train.py:203
           Loss:2.8057                                                          
[01:00:30] Epoch 036  Loss: 2.8050  L_pred:2.8011  L_time:0.0077    train.py:203
           Loss:2.8050                                                          
[01:01:34] Epoch 037  Loss: 2.8042  L_pred:2.8007  L_time:0.0070    train.py:203
           Loss:2.8042                                                          
[01:02:39] Epoch 038  Loss: 2.8036  L_pred:2.8005  L_time:0.0062    train.py:203
           Loss:2.8036                                                          
[01:03:43] Epoch 039  Loss: 2.8030  L_pred:2.8002  L_time:0.0055    train.py:203
           Loss:2.8030                                                          
[01:04:47] Epoch 040  Loss: 2.8025  L_pred:2.8001  L_time:0.0050    train.py:203
           Loss:2.8025                                                          
[01:05:51] Epoch 041  Loss: 2.8019  L_pred:2.7996  L_time:0.0046    train.py:203
           Loss:2.8019                                                          
[01:06:56] Epoch 042  Loss: 2.8014  L_pred:2.7993  L_time:0.0044    train.py:203
           Loss:2.8014                                                          
[01:08:00] Epoch 043  Loss: 2.8010  L_pred:2.7988  L_time:0.0042    train.py:203
           Loss:2.8010                                                          
[01:09:04] Epoch 044  Loss: 2.8006  L_pred:2.7986  L_time:0.0040    train.py:203
           Loss:2.8006                                                          
[01:10:08] Epoch 045  Loss: 2.8002  L_pred:2.7984  L_time:0.0037    train.py:203
           Loss:2.8002                                                          
[01:11:12] Epoch 046  Loss: 2.7999  L_pred:2.7982  L_time:0.0034    train.py:203
           Loss:2.7999                                                          
[01:12:16] Epoch 047  Loss: 2.7997  L_pred:2.7981  L_time:0.0031    train.py:203
           Loss:2.7997                                                          
[01:13:20] Epoch 048  Loss: 2.7994  L_pred:2.7978  L_time:0.0031    train.py:203
           Loss:2.7994                                                          
[01:14:25] Epoch 049  Loss: 2.7990  L_pred:2.7975  L_time:0.0030    train.py:203
           Loss:2.7990                                                          
[01:15:29] Epoch 050  Loss: 2.7989  L_pred:2.7974  L_time:0.0029    train.py:203
           Loss:2.7989                                                          
Eval Epoch 050 (team) MALE [0.4962494373321533, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5776361227035522, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch050_male0_0.4962_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
[01:22:08] Epoch 051  Loss: 2.7986  L_pred:2.7972  L_time:0.0027    train.py:203
           Loss:2.7986                                                          
[01:23:13] Epoch 052  Loss: 2.7984  L_pred:2.7971  L_time:0.0026    train.py:203
           Loss:2.7984                                                          
[01:24:17] Epoch 053  Loss: 2.7982  L_pred:2.7969  L_time:0.0026    train.py:203
           Loss:2.7982                                                          
[01:25:21] Epoch 054  Loss: 2.7979  L_pred:2.7966  L_time:0.0025    train.py:203
           Loss:2.7979                                                          
[01:26:25] Epoch 055  Loss: 2.7977  L_pred:2.7964  L_time:0.0025    train.py:203
           Loss:2.7977                                                          
[01:27:29] Epoch 056  Loss: 2.7975  L_pred:2.7962  L_time:0.0025    train.py:203
           Loss:2.7975                                                          
[01:28:33] Epoch 057  Loss: 2.7972  L_pred:2.7960  L_time:0.0024    train.py:203
           Loss:2.7972                                                          
[01:29:37] Epoch 058  Loss: 2.7970  L_pred:2.7958  L_time:0.0024    train.py:203
           Loss:2.7970                                                          
[01:30:41] Epoch 059  Loss: 2.7968  L_pred:2.7956  L_time:0.0025    train.py:203
           Loss:2.7968                                                          
[01:31:45] Epoch 060  Loss: 2.7966  L_pred:2.7953  L_time:0.0025    train.py:203
           Loss:2.7966                                                          
[01:32:48] Epoch 061  Loss: 2.7963  L_pred:2.7952  L_time:0.0024    train.py:203
           Loss:2.7963                                                          
[01:33:52] Epoch 062  Loss: 2.7960  L_pred:2.7949  L_time:0.0023    train.py:203
           Loss:2.7960                                                          
[01:34:56] Epoch 063  Loss: 2.7959  L_pred:2.7947  L_time:0.0024    train.py:203
           Loss:2.7959                                                          
[01:36:00] Epoch 064  Loss: 2.7956  L_pred:2.7944  L_time:0.0025    train.py:203
           Loss:2.7956                                                          
[01:37:04] Epoch 065  Loss: 2.7953  L_pred:2.7941  L_time:0.0024    train.py:203
           Loss:2.7953                                                          
[01:38:08] Epoch 066  Loss: 2.7950  L_pred:2.7938  L_time:0.0023    train.py:203
           Loss:2.7950                                                          
[01:39:13] Epoch 067  Loss: 2.7949  L_pred:2.7936  L_time:0.0025    train.py:203
           Loss:2.7949                                                          
[01:40:17] Epoch 068  Loss: 2.7946  L_pred:2.7933  L_time:0.0025    train.py:203
           Loss:2.7946                                                          
[01:41:21] Epoch 069  Loss: 2.7943  L_pred:2.7931  L_time:0.0024    train.py:203
           Loss:2.7943                                                          
[01:42:25] Epoch 070  Loss: 2.7942  L_pred:2.7929  L_time:0.0025    train.py:203
           Loss:2.7942                                                          
[01:43:29] Epoch 071  Loss: 2.7939  L_pred:2.7926  L_time:0.0025    train.py:203
           Loss:2.7939                                                          
[01:44:33] Epoch 072  Loss: 2.7937  L_pred:2.7925  L_time:0.0025    train.py:203
           Loss:2.7937                                                          
[01:45:37] Epoch 073  Loss: 2.7934  L_pred:2.7922  L_time:0.0025    train.py:203
           Loss:2.7934                                                          
[01:46:41] Epoch 074  Loss: 2.7933  L_pred:2.7921  L_time:0.0025    train.py:203
           Loss:2.7933                                                          
[01:47:45] Epoch 075  Loss: 2.7931  L_pred:2.7920  L_time:0.0023    train.py:203
           Loss:2.7931                                                          
Eval Epoch 075 (team) MALE [0.46471622586250305, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5440837144851685, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch075_male0_0.4647_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
[01:54:24] Epoch 076  Loss: 2.7928  L_pred:2.7917  L_time:0.0023    train.py:203
           Loss:2.7928                                                          
[01:55:28] Epoch 077  Loss: 2.7926  L_pred:2.7915  L_time:0.0022    train.py:203
           Loss:2.7926                                                          
[01:56:32] Epoch 078  Loss: 2.7924  L_pred:2.7914  L_time:0.0022    train.py:203
           Loss:2.7924                                                          
[01:57:36] Epoch 079  Loss: 2.7922  L_pred:2.7911  L_time:0.0022    train.py:203
           Loss:2.7922                                                          
[01:58:40] Epoch 080  Loss: 2.7919  L_pred:2.7909  L_time:0.0021    train.py:203
           Loss:2.7919                                                          
[01:59:44] Epoch 081  Loss: 2.7917  L_pred:2.7907  L_time:0.0021    train.py:203
           Loss:2.7917                                                          
[02:00:48] Epoch 082  Loss: 2.7915  L_pred:2.7904  L_time:0.0022    train.py:203
           Loss:2.7915                                                          
[02:01:53] Epoch 083  Loss: 2.7912  L_pred:2.7901  L_time:0.0021    train.py:203
           Loss:2.7912                                                          
[02:02:57] Epoch 084  Loss: 2.7911  L_pred:2.7899  L_time:0.0023    train.py:203
           Loss:2.7911                                                          
[02:04:01] Epoch 085  Loss: 2.7909  L_pred:2.7898  L_time:0.0021    train.py:203
           Loss:2.7909                                                          
[02:05:06] Epoch 086  Loss: 2.7906  L_pred:2.7894  L_time:0.0023    train.py:203
           Loss:2.7906                                                          
[02:06:10] Epoch 087  Loss: 2.7903  L_pred:2.7892  L_time:0.0022    train.py:203
           Loss:2.7903                                                          
[02:07:14] Epoch 088  Loss: 2.7902  L_pred:2.7892  L_time:0.0020    train.py:203
           Loss:2.7902                                                          
[02:08:18] Epoch 089  Loss: 2.7900  L_pred:2.7889  L_time:0.0022    train.py:203
           Loss:2.7900                                                          
[02:09:22] Epoch 090  Loss: 2.7898  L_pred:2.7888  L_time:0.0020    train.py:203
           Loss:2.7898                                                          
[02:10:26] Epoch 091  Loss: 2.7896  L_pred:2.7886  L_time:0.0020    train.py:203
           Loss:2.7896                                                          
[02:11:31] Epoch 092  Loss: 2.7895  L_pred:2.7884  L_time:0.0021    train.py:203
           Loss:2.7895                                                          
[02:12:35] Epoch 093  Loss: 2.7894  L_pred:2.7885  L_time:0.0019    train.py:203
           Loss:2.7894                                                          
[02:13:39] Epoch 094  Loss: 2.7890  L_pred:2.7880  L_time:0.0020    train.py:203
           Loss:2.7890                                                          
[02:14:43] Epoch 095  Loss: 2.7888  L_pred:2.7878  L_time:0.0020    train.py:203
           Loss:2.7888                                                          
[02:15:47] Epoch 096  Loss: 2.7886  L_pred:2.7877  L_time:0.0018    train.py:203
           Loss:2.7886                                                          
[02:16:51] Epoch 097  Loss: 2.7884  L_pred:2.7874  L_time:0.0019    train.py:203
           Loss:2.7884                                                          
[02:17:55] Epoch 098  Loss: 2.7881  L_pred:2.7872  L_time:0.0018    train.py:203
           Loss:2.7881                                                          
[02:18:59] Epoch 099  Loss: 2.7880  L_pred:2.7871  L_time:0.0018    train.py:203
           Loss:2.7880                                                          
[02:20:03] Epoch 100  Loss: 2.7878  L_pred:2.7869  L_time:0.0019    train.py:203
           Loss:2.7878                                                          
Eval Epoch 100 (team) MALE [0.45239344239234924, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5288569927215576, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch100_male0_0.4524_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
[02:26:42] Epoch 101  Loss: 2.7876  L_pred:2.7867  L_time:0.0018    train.py:203
           Loss:2.7876                                                          
[02:27:46] Epoch 102  Loss: 2.7873  L_pred:2.7865  L_time:0.0017    train.py:203
           Loss:2.7873                                                          
[02:28:50] Epoch 103  Loss: 2.7873  L_pred:2.7864  L_time:0.0018    train.py:203
           Loss:2.7873                                                          
[02:29:55] Epoch 104  Loss: 2.7871  L_pred:2.7863  L_time:0.0017    train.py:203
           Loss:2.7871                                                          
[02:30:59] Epoch 105  Loss: 2.7868  L_pred:2.7859  L_time:0.0018    train.py:203
           Loss:2.7868                                                          
[02:32:03] Epoch 106  Loss: 2.7865  L_pred:2.7857  L_time:0.0017    train.py:203
           Loss:2.7865                                                          
[02:33:08] Epoch 107  Loss: 2.7865  L_pred:2.7857  L_time:0.0016    train.py:203
           Loss:2.7865                                                          
[02:34:12] Epoch 108  Loss: 2.7863  L_pred:2.7855  L_time:0.0017    train.py:203
           Loss:2.7863                                                          
[02:35:17] Epoch 109  Loss: 2.7860  L_pred:2.7852  L_time:0.0016    train.py:203
           Loss:2.7860                                                          
[02:36:21] Epoch 110  Loss: 2.7857  L_pred:2.7849  L_time:0.0016    train.py:203
           Loss:2.7857                                                          
[02:37:26] Epoch 111  Loss: 2.7856  L_pred:2.7849  L_time:0.0016    train.py:203
           Loss:2.7856                                                          
[02:38:31] Epoch 112  Loss: 2.7856  L_pred:2.7848  L_time:0.0015    train.py:203
           Loss:2.7856                                                          
[02:39:35] Epoch 113  Loss: 2.7855  L_pred:2.7847  L_time:0.0017    train.py:203
           Loss:2.7855                                                          
[02:40:40] Epoch 114  Loss: 2.7852  L_pred:2.7845  L_time:0.0015    train.py:203
           Loss:2.7852                                                          
[02:41:44] Epoch 115  Loss: 2.7848  L_pred:2.7840  L_time:0.0015    train.py:203
           Loss:2.7848                                                          
[02:42:49] Epoch 116  Loss: 2.7847  L_pred:2.7839  L_time:0.0016    train.py:203
           Loss:2.7847                                                          
[02:43:53] Epoch 117  Loss: 2.7852  L_pred:2.7845  L_time:0.0014    train.py:203
           Loss:2.7852                                                          
[02:44:58] Epoch 118  Loss: 2.7846  L_pred:2.7838  L_time:0.0016    train.py:203
           Loss:2.7846                                                          
[02:46:02] Epoch 119  Loss: 2.7840  L_pred:2.7833  L_time:0.0014    train.py:203
           Loss:2.7840                                                          
[02:47:06] Epoch 120  Loss: 2.7840  L_pred:2.7833  L_time:0.0013    train.py:203
           Loss:2.7840                                                          
[02:48:11] Epoch 121  Loss: 2.7840  L_pred:2.7832  L_time:0.0015    train.py:203
           Loss:2.7840                                                          
[02:49:14] Epoch 122  Loss: 2.7836  L_pred:2.7830  L_time:0.0014    train.py:203
           Loss:2.7836                                                          
[02:50:18] Epoch 123  Loss: 2.7832  L_pred:2.7825  L_time:0.0014    train.py:203
           Loss:2.7832                                                          
[02:51:22] Epoch 124  Loss: 2.7833  L_pred:2.7826  L_time:0.0014    train.py:203
           Loss:2.7833                                                          
[02:52:26] Epoch 125  Loss: 2.7836  L_pred:2.7829  L_time:0.0013    train.py:203
           Loss:2.7836                                                          
Eval Epoch 125 (team) MALE [0.44916340708732605, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5216690301895142, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch125_male0_0.4492_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch025_male0.4510_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch125_male0.4492_team.pt (MALE: 0.4492 
at epoch 125)
[02:59:10] Epoch 126  Loss: 2.7830  L_pred:2.7822  L_time:0.0015    train.py:203
           Loss:2.7830                                                          
[03:00:14] Epoch 127  Loss: 2.7824  L_pred:2.7817  L_time:0.0014    train.py:203
           Loss:2.7824                                                          
[03:01:18] Epoch 128  Loss: 2.7826  L_pred:2.7820  L_time:0.0013    train.py:203
           Loss:2.7826                                                          
[03:02:23] Epoch 129  Loss: 2.7825  L_pred:2.7818  L_time:0.0015    train.py:203
           Loss:2.7825                                                          
[03:03:27] Epoch 130  Loss: 2.7823  L_pred:2.7816  L_time:0.0013    train.py:203
           Loss:2.7823                                                          
[03:04:31] Epoch 131  Loss: 2.7818  L_pred:2.7811  L_time:0.0014    train.py:203
           Loss:2.7818                                                          
[03:05:35] Epoch 132  Loss: 2.7816  L_pred:2.7809  L_time:0.0014    train.py:203
           Loss:2.7816                                                          
[03:06:39] Epoch 133  Loss: 2.7817  L_pred:2.7811  L_time:0.0013    train.py:203
           Loss:2.7817                                                          
[03:07:44] Epoch 134  Loss: 2.7813  L_pred:2.7806  L_time:0.0014    train.py:203
           Loss:2.7813                                                          
[03:08:48] Epoch 135  Loss: 2.7810  L_pred:2.7803  L_time:0.0014    train.py:203
           Loss:2.7810                                                          
[03:09:52] Epoch 136  Loss: 2.7807  L_pred:2.7800  L_time:0.0013    train.py:203
           Loss:2.7807                                                          
[03:10:56] Epoch 137  Loss: 2.7808  L_pred:2.7801  L_time:0.0014    train.py:203
           Loss:2.7808                                                          
[03:12:00] Epoch 138  Loss: 2.7808  L_pred:2.7802  L_time:0.0013    train.py:203
           Loss:2.7808                                                          
[03:13:05] Epoch 139  Loss: 2.7808  L_pred:2.7800  L_time:0.0015    train.py:203
           Loss:2.7808                                                          
[03:14:09] Epoch 140  Loss: 2.7810  L_pred:2.7804  L_time:0.0013    train.py:203
           Loss:2.7810                                                          
[03:15:12] Epoch 141  Loss: 2.7803  L_pred:2.7796  L_time:0.0014    train.py:203
           Loss:2.7803                                                          
[03:16:16] Epoch 142  Loss: 2.7797  L_pred:2.7790  L_time:0.0014    train.py:203
           Loss:2.7797                                                          
[03:17:20] Epoch 143  Loss: 2.7798  L_pred:2.7791  L_time:0.0013    train.py:203
           Loss:2.7798                                                          
[03:18:25] Epoch 144  Loss: 2.7797  L_pred:2.7790  L_time:0.0014    train.py:203
           Loss:2.7797                                                          
[03:19:29] Epoch 145  Loss: 2.7795  L_pred:2.7789  L_time:0.0012    train.py:203
           Loss:2.7795                                                          
[03:20:34] Epoch 146  Loss: 2.7790  L_pred:2.7784  L_time:0.0013    train.py:203
           Loss:2.7790                                                          
[03:21:39] Epoch 147  Loss: 2.7790  L_pred:2.7783  L_time:0.0014    train.py:203
           Loss:2.7790                                                          
[03:22:43] Epoch 148  Loss: 2.7793  L_pred:2.7787  L_time:0.0013    train.py:203
           Loss:2.7793                                                          
[03:23:48] Epoch 149  Loss: 2.7793  L_pred:2.7786  L_time:0.0014    train.py:203
           Loss:2.7793                                                          
[03:24:52] Epoch 150  Loss: 2.7787  L_pred:2.7781  L_time:0.0013    train.py:203
           Loss:2.7787                                                          
Eval Epoch 150 (team) MALE [0.43033722043037415, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.5019323229789734, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch150_male0_0.4303_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch125_male0.4492_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch150_male0.4303_team.pt (MALE: 0.4303 
at epoch 150)
[03:31:33] Epoch 151  Loss: 2.7783  L_pred:2.7776  L_time:0.0013    train.py:203
           Loss:2.7783                                                          
[03:32:37] Epoch 152  Loss: 2.7781  L_pred:2.7774  L_time:0.0013    train.py:203
           Loss:2.7781                                                          
[03:33:41] Epoch 153  Loss: 2.7781  L_pred:2.7775  L_time:0.0012    train.py:203
           Loss:2.7781                                                          
[03:34:46] Epoch 154  Loss: 2.7779  L_pred:2.7772  L_time:0.0014    train.py:203
           Loss:2.7779                                                          
[03:35:50] Epoch 155  Loss: 2.7774  L_pred:2.7767  L_time:0.0013    train.py:203
           Loss:2.7774                                                          
[03:36:55] Epoch 156  Loss: 2.7773  L_pred:2.7766  L_time:0.0013    train.py:203
           Loss:2.7773                                                          
[03:37:59] Epoch 157  Loss: 2.7773  L_pred:2.7766  L_time:0.0013    train.py:203
           Loss:2.7773                                                          
[03:39:03] Epoch 158  Loss: 2.7770  L_pred:2.7763  L_time:0.0013    train.py:203
           Loss:2.7770                                                          
[03:40:08] Epoch 159  Loss: 2.7770  L_pred:2.7763  L_time:0.0014    train.py:203
           Loss:2.7770                                                          
[03:41:12] Epoch 160  Loss: 2.7769  L_pred:2.7763  L_time:0.0013    train.py:203
           Loss:2.7769                                                          
[03:42:17] Epoch 161  Loss: 2.7768  L_pred:2.7761  L_time:0.0014    train.py:203
           Loss:2.7768                                                          
[03:43:21] Epoch 162  Loss: 2.7766  L_pred:2.7759  L_time:0.0013    train.py:203
           Loss:2.7766                                                          
[03:44:25] Epoch 163  Loss: 2.7767  L_pred:2.7760  L_time:0.0014    train.py:203
           Loss:2.7767                                                          
[03:45:30] Epoch 164  Loss: 2.7764  L_pred:2.7758  L_time:0.0012    train.py:203
           Loss:2.7764                                                          
[03:46:34] Epoch 165  Loss: 2.7761  L_pred:2.7755  L_time:0.0013    train.py:203
           Loss:2.7761                                                          
[03:47:39] Epoch 166  Loss: 2.7759  L_pred:2.7752  L_time:0.0013    train.py:203
           Loss:2.7759                                                          
[03:48:43] Epoch 167  Loss: 2.7757  L_pred:2.7750  L_time:0.0014    train.py:203
           Loss:2.7757                                                          
[03:49:47] Epoch 168  Loss: 2.7756  L_pred:2.7749  L_time:0.0013    train.py:203
           Loss:2.7756                                                          
[03:50:51] Epoch 169  Loss: 2.7754  L_pred:2.7748  L_time:0.0013    train.py:203
           Loss:2.7754                                                          
[03:51:56] Epoch 170  Loss: 2.7751  L_pred:2.7744  L_time:0.0014    train.py:203
           Loss:2.7751                                                          
[03:53:00] Epoch 171  Loss: 2.7750  L_pred:2.7744  L_time:0.0013    train.py:203
           Loss:2.7750                                                          
[03:54:05] Epoch 172  Loss: 2.7748  L_pred:2.7741  L_time:0.0013    train.py:203
           Loss:2.7748                                                          
[03:55:09] Epoch 173  Loss: 2.7746  L_pred:2.7740  L_time:0.0013    train.py:203
           Loss:2.7746                                                          
[03:56:13] Epoch 174  Loss: 2.7746  L_pred:2.7739  L_time:0.0014    train.py:203
           Loss:2.7746                                                          
[03:57:18] Epoch 175  Loss: 2.7743  L_pred:2.7736  L_time:0.0014    train.py:203
           Loss:2.7743                                                          
Eval Epoch 175 (team) MALE [0.4264407157897949, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.4971097707748413, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch175_male0_0.4264_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch150_male0.4303_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch175_male0.4264_team.pt (MALE: 0.4264 
at epoch 175)
[04:04:01] Epoch 176  Loss: 2.7744  L_pred:2.7737  L_time:0.0013    train.py:203
           Loss:2.7744                                                          
[04:05:05] Epoch 177  Loss: 2.7741  L_pred:2.7734  L_time:0.0014    train.py:203
           Loss:2.7741                                                          
[04:06:09] Epoch 178  Loss: 2.7748  L_pred:2.7741  L_time:0.0013    train.py:203
           Loss:2.7748                                                          
[04:07:13] Epoch 179  Loss: 2.7758  L_pred:2.7750  L_time:0.0015    train.py:203
           Loss:2.7758                                                          
[04:08:17] Epoch 180  Loss: 2.7782  L_pred:2.7776  L_time:0.0012    train.py:203
           Loss:2.7782                                                          
[04:09:22] Epoch 181  Loss: 2.7736  L_pred:2.7730  L_time:0.0012    train.py:203
           Loss:2.7736                                                          
[04:10:25] Epoch 182  Loss: 2.7770  L_pred:2.7763  L_time:0.0014    train.py:203
           Loss:2.7770                                                          
[04:11:29] Epoch 183  Loss: 2.7786  L_pred:2.7781  L_time:0.0011    train.py:203
           Loss:2.7786                                                          
[04:12:32] Epoch 184  Loss: 2.7752  L_pred:2.7746  L_time:0.0011    train.py:203
           Loss:2.7752                                                          
[04:13:36] Epoch 185  Loss: 2.7776  L_pred:2.7769  L_time:0.0013    train.py:203
           Loss:2.7776                                                          
[04:14:40] Epoch 186  Loss: 2.7732  L_pred:2.7726  L_time:0.0012    train.py:203
           Loss:2.7732                                                          
[04:15:44] Epoch 187  Loss: 2.7755  L_pred:2.7750  L_time:0.0011    train.py:203
           Loss:2.7755                                                          
[04:16:49] Epoch 188  Loss: 2.7740  L_pred:2.7735  L_time:0.0011    train.py:203
           Loss:2.7740                                                          
[04:17:53] Epoch 189  Loss: 2.7741  L_pred:2.7735  L_time:0.0013    train.py:203
           Loss:2.7741                                                          
[04:18:57] Epoch 190  Loss: 2.7736  L_pred:2.7729  L_time:0.0013    train.py:203
           Loss:2.7736                                                          
[04:20:01] Epoch 191  Loss: 2.7736  L_pred:2.7730  L_time:0.0011    train.py:203
           Loss:2.7736                                                          
[04:21:05] Epoch 192  Loss: 2.7737  L_pred:2.7731  L_time:0.0012    train.py:203
           Loss:2.7737                                                          
[04:22:09] Epoch 193  Loss: 2.7726  L_pred:2.7719  L_time:0.0014    train.py:203
           Loss:2.7726                                                          
[04:23:14] Epoch 194  Loss: 2.7734  L_pred:2.7727  L_time:0.0015    train.py:203
           Loss:2.7734                                                          
[04:24:18] Epoch 195  Loss: 2.7724  L_pred:2.7718  L_time:0.0013    train.py:203
           Loss:2.7724                                                          
[04:25:22] Epoch 196  Loss: 2.7730  L_pred:2.7724  L_time:0.0013    train.py:203
           Loss:2.7730                                                          
[04:26:26] Epoch 197  Loss: 2.7722  L_pred:2.7715  L_time:0.0014    train.py:203
           Loss:2.7722                                                          
[04:27:30] Epoch 198  Loss: 2.7724  L_pred:2.7717  L_time:0.0015    train.py:203
           Loss:2.7724                                                          
[04:28:34] Epoch 199  Loss: 2.7719  L_pred:2.7711  L_time:0.0015    train.py:203
           Loss:2.7719                                                          
[04:29:38] Epoch 200  Loss: 2.7721  L_pred:2.7714  L_time:0.0014    train.py:203
           Loss:2.7721                                                          
Eval Epoch 200 (team) MALE [0.42490890622138977, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.49348539113998413, 1.1996724605560303, 1.3402553796768188, 
1.3455837965011597, 1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch200_male0_0.4249_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch175_male0.4264_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch200_male0.4249_team.pt (MALE: 0.4249 
at epoch 200)
[04:36:17] Epoch 201  Loss: 2.7715  L_pred:2.7708  L_time:0.0014    train.py:203
           Loss:2.7715                                                          
[04:37:20] Epoch 202  Loss: 2.7722  L_pred:2.7715  L_time:0.0014    train.py:203
           Loss:2.7722                                                          
[04:38:24] Epoch 203  Loss: 2.7710  L_pred:2.7703  L_time:0.0014    train.py:203
           Loss:2.7710                                                          
[04:39:28] Epoch 204  Loss: 2.7717  L_pred:2.7710  L_time:0.0014    train.py:203
           Loss:2.7717                                                          
[04:40:33] Epoch 205  Loss: 2.7710  L_pred:2.7703  L_time:0.0015    train.py:203
           Loss:2.7710                                                          
[04:41:37] Epoch 206  Loss: 2.7713  L_pred:2.7705  L_time:0.0015    train.py:203
           Loss:2.7713                                                          
[04:42:41] Epoch 207  Loss: 2.7708  L_pred:2.7701  L_time:0.0014    train.py:203
           Loss:2.7708                                                          
[04:43:45] Epoch 208  Loss: 2.7709  L_pred:2.7702  L_time:0.0014    train.py:203
           Loss:2.7709                                                          
[04:44:49] Epoch 209  Loss: 2.7704  L_pred:2.7697  L_time:0.0015    train.py:203
           Loss:2.7704                                                          
[04:45:53] Epoch 210  Loss: 2.7706  L_pred:2.7698  L_time:0.0016    train.py:203
           Loss:2.7706                                                          
[04:46:57] Epoch 211  Loss: 2.7702  L_pred:2.7695  L_time:0.0015    train.py:203
           Loss:2.7702                                                          
[04:48:02] Epoch 212  Loss: 2.7702  L_pred:2.7695  L_time:0.0014    train.py:203
           Loss:2.7702                                                          
[04:49:06] Epoch 213  Loss: 2.7700  L_pred:2.7692  L_time:0.0015    train.py:203
           Loss:2.7700                                                          
[04:50:10] Epoch 214  Loss: 2.7698  L_pred:2.7690  L_time:0.0015    train.py:203
           Loss:2.7698                                                          
[04:51:14] Epoch 215  Loss: 2.7697  L_pred:2.7689  L_time:0.0015    train.py:203
           Loss:2.7697                                                          
[04:52:18] Epoch 216  Loss: 2.7695  L_pred:2.7688  L_time:0.0015    train.py:203
           Loss:2.7695                                                          
[04:53:23] Epoch 217  Loss: 2.7696  L_pred:2.7689  L_time:0.0015    train.py:203
           Loss:2.7696                                                          
[04:54:27] Epoch 218  Loss: 2.7694  L_pred:2.7687  L_time:0.0014    train.py:203
           Loss:2.7694                                                          
[04:55:31] Epoch 219  Loss: 2.7693  L_pred:2.7686  L_time:0.0015    train.py:203
           Loss:2.7693                                                          
[04:56:35] Epoch 220  Loss: 2.7691  L_pred:2.7684  L_time:0.0015    train.py:203
           Loss:2.7691                                                          
[04:57:39] Epoch 221  Loss: 2.7688  L_pred:2.7681  L_time:0.0015    train.py:203
           Loss:2.7688                                                          
[04:58:43] Epoch 222  Loss: 2.7688  L_pred:2.7680  L_time:0.0014    train.py:203
           Loss:2.7688                                                          
[04:59:47] Epoch 223  Loss: 2.7687  L_pred:2.7680  L_time:0.0015    train.py:203
           Loss:2.7687                                                          
[05:00:51] Epoch 224  Loss: 2.7686  L_pred:2.7679  L_time:0.0015    train.py:203
           Loss:2.7686                                                          
[05:01:56] Epoch 225  Loss: 2.7685  L_pred:2.7677  L_time:0.0015    train.py:203
           Loss:2.7685                                                          
Eval Epoch 225 (team) MALE [0.4065505862236023, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.4757970869541168, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch225_male0_0.4066_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch200_male0.4249_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch225_male0.4066_team.pt (MALE: 0.4066 
at epoch 225)
[05:08:35] Epoch 226  Loss: 2.7685  L_pred:2.7678  L_time:0.0015    train.py:203
           Loss:2.7685                                                          
[05:09:39] Epoch 227  Loss: 2.7681  L_pred:2.7673  L_time:0.0015    train.py:203
           Loss:2.7681                                                          
[05:10:44] Epoch 228  Loss: 2.7680  L_pred:2.7673  L_time:0.0015    train.py:203
           Loss:2.7680                                                          
[05:11:48] Epoch 229  Loss: 2.7681  L_pred:2.7674  L_time:0.0015    train.py:203
           Loss:2.7681                                                          
[05:12:52] Epoch 230  Loss: 2.7678  L_pred:2.7671  L_time:0.0015    train.py:203
           Loss:2.7678                                                          
[05:13:56] Epoch 231  Loss: 2.7678  L_pred:2.7671  L_time:0.0015    train.py:203
           Loss:2.7678                                                          
[05:15:00] Epoch 232  Loss: 2.7677  L_pred:2.7669  L_time:0.0015    train.py:203
           Loss:2.7677                                                          
[05:16:04] Epoch 233  Loss: 2.7674  L_pred:2.7666  L_time:0.0015    train.py:203
           Loss:2.7674                                                          
[05:17:08] Epoch 234  Loss: 2.7673  L_pred:2.7666  L_time:0.0015    train.py:203
           Loss:2.7673                                                          
[05:18:12] Epoch 235  Loss: 2.7672  L_pred:2.7664  L_time:0.0015    train.py:203
           Loss:2.7672                                                          
[05:19:16] Epoch 236  Loss: 2.7671  L_pred:2.7663  L_time:0.0015    train.py:203
           Loss:2.7671                                                          
[05:20:20] Epoch 237  Loss: 2.7669  L_pred:2.7662  L_time:0.0015    train.py:203
           Loss:2.7669                                                          
[05:21:25] Epoch 238  Loss: 2.7668  L_pred:2.7660  L_time:0.0015    train.py:203
           Loss:2.7668                                                          
[05:22:29] Epoch 239  Loss: 2.7669  L_pred:2.7662  L_time:0.0015    train.py:203
           Loss:2.7669                                                          
[05:23:33] Epoch 240  Loss: 2.7667  L_pred:2.7659  L_time:0.0015    train.py:203
           Loss:2.7667                                                          
[05:24:38] Epoch 241  Loss: 2.7666  L_pred:2.7659  L_time:0.0015    train.py:203
           Loss:2.7666                                                          
[05:25:43] Epoch 242  Loss: 2.7664  L_pred:2.7657  L_time:0.0015    train.py:203
           Loss:2.7664                                                          
[05:26:47] Epoch 243  Loss: 2.7662  L_pred:2.7655  L_time:0.0015    train.py:203
           Loss:2.7662                                                          
[05:27:51] Epoch 244  Loss: 2.7662  L_pred:2.7654  L_time:0.0015    train.py:203
           Loss:2.7662                                                          
[05:28:55] Epoch 245  Loss: 2.7661  L_pred:2.7654  L_time:0.0015    train.py:203
           Loss:2.7661                                                          
[05:30:00] Epoch 246  Loss: 2.7660  L_pred:2.7653  L_time:0.0015    train.py:203
           Loss:2.7660                                                          
[05:31:04] Epoch 247  Loss: 2.7658  L_pred:2.7651  L_time:0.0015    train.py:203
           Loss:2.7658                                                          
[05:32:08] Epoch 248  Loss: 2.7657  L_pred:2.7650  L_time:0.0015    train.py:203
           Loss:2.7657                                                          
[05:33:12] Epoch 249  Loss: 2.7656  L_pred:2.7649  L_time:0.0015    train.py:203
           Loss:2.7656                                                          
[05:34:16] Epoch 250  Loss: 2.7655  L_pred:2.7648  L_time:0.0015    train.py:203
           Loss:2.7655                                                          
Eval Epoch 250 (team) MALE [0.404107928276062, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.4727266728878021, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch250_male0_0.4041_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch225_male0.4066_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch250_male0.4041_team.pt (MALE: 0.4041 
at epoch 250)
[05:41:00] Epoch 251  Loss: 2.7654  L_pred:2.7647  L_time:0.0015    train.py:203
           Loss:2.7654                                                          
[05:42:05] Epoch 252  Loss: 2.7653  L_pred:2.7646  L_time:0.0015    train.py:203
           Loss:2.7653                                                          
[05:43:10] Epoch 253  Loss: 2.7653  L_pred:2.7646  L_time:0.0015    train.py:203
           Loss:2.7653                                                          
[05:44:15] Epoch 254  Loss: 2.7651  L_pred:2.7644  L_time:0.0015    train.py:203
           Loss:2.7651                                                          
[05:45:20] Epoch 255  Loss: 2.7650  L_pred:2.7643  L_time:0.0015    train.py:203
           Loss:2.7650                                                          
[05:46:24] Epoch 256  Loss: 2.7650  L_pred:2.7642  L_time:0.0015    train.py:203
           Loss:2.7650                                                          
[05:47:29] Epoch 257  Loss: 2.7649  L_pred:2.7641  L_time:0.0015    train.py:203
           Loss:2.7649                                                          
[05:48:33] Epoch 258  Loss: 2.7647  L_pred:2.7640  L_time:0.0015    train.py:203
           Loss:2.7647                                                          
[05:49:37] Epoch 259  Loss: 2.7646  L_pred:2.7639  L_time:0.0014    train.py:203
           Loss:2.7646                                                          
[05:50:42] Epoch 260  Loss: 2.7647  L_pred:2.7640  L_time:0.0015    train.py:203
           Loss:2.7647                                                          
[05:51:46] Epoch 261  Loss: 2.7647  L_pred:2.7640  L_time:0.0014    train.py:203
           Loss:2.7647                                                          
[05:52:50] Epoch 262  Loss: 2.7644  L_pred:2.7636  L_time:0.0015    train.py:203
           Loss:2.7644                                                          
[05:53:54] Epoch 263  Loss: 2.7640  L_pred:2.7633  L_time:0.0014    train.py:203
           Loss:2.7640                                                          
[05:54:58] Epoch 264  Loss: 2.7640  L_pred:2.7633  L_time:0.0014    train.py:203
           Loss:2.7640                                                          
[05:56:02] Epoch 265  Loss: 2.7642  L_pred:2.7635  L_time:0.0014    train.py:203
           Loss:2.7642                                                          
[05:57:06] Epoch 266  Loss: 2.7642  L_pred:2.7635  L_time:0.0014    train.py:203
           Loss:2.7642                                                          
[05:58:10] Epoch 267  Loss: 2.7640  L_pred:2.7633  L_time:0.0015    train.py:203
           Loss:2.7640                                                          
[05:59:14] Epoch 268  Loss: 2.7637  L_pred:2.7630  L_time:0.0014    train.py:203
           Loss:2.7637                                                          
[06:00:19] Epoch 269  Loss: 2.7635  L_pred:2.7628  L_time:0.0014    train.py:203
           Loss:2.7635                                                          
[06:01:23] Epoch 270  Loss: 2.7635  L_pred:2.7628  L_time:0.0014    train.py:203
           Loss:2.7635                                                          
[06:02:27] Epoch 271  Loss: 2.7636  L_pred:2.7629  L_time:0.0014    train.py:203
           Loss:2.7636                                                          
[06:03:31] Epoch 272  Loss: 2.7638  L_pred:2.7631  L_time:0.0014    train.py:203
           Loss:2.7638                                                          
[06:04:35] Epoch 273  Loss: 2.7640  L_pred:2.7633  L_time:0.0014    train.py:203
           Loss:2.7640                                                          
[06:05:39] Epoch 274  Loss: 2.7635  L_pred:2.7627  L_time:0.0014    train.py:203
           Loss:2.7635                                                          
[06:06:43] Epoch 275  Loss: 2.7631  L_pred:2.7625  L_time:0.0014    train.py:203
           Loss:2.7631                                                          
Eval Epoch 275 (team) MALE [0.39589887857437134, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.46369272470474243, 1.1996724605560303, 1.3402553796768188, 
1.3455837965011597, 1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch275_male0_0.3959_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch250_male0.4041_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch275_male0.3959_team.pt (MALE: 0.3959 
at epoch 275)
[06:13:22] Epoch 276  Loss: 2.7629  L_pred:2.7622  L_time:0.0014    train.py:203
           Loss:2.7629                                                          
[06:14:26] Epoch 277  Loss: 2.7631  L_pred:2.7624  L_time:0.0014    train.py:203
           Loss:2.7631                                                          
[06:15:30] Epoch 278  Loss: 2.7635  L_pred:2.7628  L_time:0.0013    train.py:203
           Loss:2.7635                                                          
[06:16:34] Epoch 279  Loss: 2.7631  L_pred:2.7624  L_time:0.0014    train.py:203
           Loss:2.7631                                                          
[06:17:39] Epoch 280  Loss: 2.7627  L_pred:2.7621  L_time:0.0013    train.py:203
           Loss:2.7627                                                          
[06:18:43] Epoch 281  Loss: 2.7626  L_pred:2.7619  L_time:0.0013    train.py:203
           Loss:2.7626                                                          
[06:19:47] Epoch 282  Loss: 2.7628  L_pred:2.7621  L_time:0.0014    train.py:203
           Loss:2.7628                                                          
[06:20:52] Epoch 283  Loss: 2.7631  L_pred:2.7625  L_time:0.0013    train.py:203
           Loss:2.7631                                                          
[06:21:56] Epoch 284  Loss: 2.7623  L_pred:2.7617  L_time:0.0013    train.py:203
           Loss:2.7623                                                          
[06:23:00] Epoch 285  Loss: 2.7623  L_pred:2.7617  L_time:0.0013    train.py:203
           Loss:2.7623                                                          
[06:24:04] Epoch 286  Loss: 2.7625  L_pred:2.7618  L_time:0.0013    train.py:203
           Loss:2.7625                                                          
[06:25:09] Epoch 287  Loss: 2.7625  L_pred:2.7618  L_time:0.0014    train.py:203
           Loss:2.7625                                                          
[06:26:13] Epoch 288  Loss: 2.7621  L_pred:2.7614  L_time:0.0013    train.py:203
           Loss:2.7621                                                          
[06:27:18] Epoch 289  Loss: 2.7620  L_pred:2.7613  L_time:0.0013    train.py:203
           Loss:2.7620                                                          
[06:28:22] Epoch 290  Loss: 2.7621  L_pred:2.7614  L_time:0.0014    train.py:203
           Loss:2.7621                                                          
[06:29:26] Epoch 291  Loss: 2.7623  L_pred:2.7617  L_time:0.0013    train.py:203
           Loss:2.7623                                                          
[06:30:30] Epoch 292  Loss: 2.7621  L_pred:2.7614  L_time:0.0013    train.py:203
           Loss:2.7621                                                          
[06:31:35] Epoch 293  Loss: 2.7618  L_pred:2.7612  L_time:0.0013    train.py:203
           Loss:2.7618                                                          
[06:32:39] Epoch 294  Loss: 2.7616  L_pred:2.7609  L_time:0.0013    train.py:203
           Loss:2.7616                                                          
[06:33:44] Epoch 295  Loss: 2.7618  L_pred:2.7611  L_time:0.0013    train.py:203
           Loss:2.7618                                                          
[06:34:48] Epoch 296  Loss: 2.7620  L_pred:2.7614  L_time:0.0013    train.py:203
           Loss:2.7620                                                          
[06:35:52] Epoch 297  Loss: 2.7616  L_pred:2.7610  L_time:0.0013    train.py:203
           Loss:2.7616                                                          
[06:36:56] Epoch 298  Loss: 2.7613  L_pred:2.7607  L_time:0.0013    train.py:203
           Loss:2.7613                                                          
[06:38:00] Epoch 299  Loss: 2.7616  L_pred:2.7610  L_time:0.0013    train.py:203
           Loss:2.7616                                                          
[06:39:04] Epoch 300  Loss: 2.7616  L_pred:2.7609  L_time:0.0013    train.py:203
           Loss:2.7616                                                          
Eval Epoch 300 (team) MALE [0.38291171193122864, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.45091962814331055, 1.1996724605560303, 1.3402553796768188, 
1.3455837965011597, 1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch300_male0_0.3829_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch275_male0.3959_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch300_male0.3829_team.pt (MALE: 0.3829 
at epoch 300)
[06:45:44] Epoch 301  Loss: 2.7615  L_pred:2.7609  L_time:0.0013    train.py:203
           Loss:2.7615                                                          
[06:46:48] Epoch 302  Loss: 2.7610  L_pred:2.7604  L_time:0.0013    train.py:203
           Loss:2.7610                                                          
[06:47:53] Epoch 303  Loss: 2.7612  L_pred:2.7605  L_time:0.0013    train.py:203
           Loss:2.7612                                                          
[06:48:57] Epoch 304  Loss: 2.7612  L_pred:2.7605  L_time:0.0012    train.py:203
           Loss:2.7612                                                          
[06:50:01] Epoch 305  Loss: 2.7612  L_pred:2.7605  L_time:0.0013    train.py:203
           Loss:2.7612                                                          
[06:51:05] Epoch 306  Loss: 2.7610  L_pred:2.7603  L_time:0.0013    train.py:203
           Loss:2.7610                                                          
[06:52:10] Epoch 307  Loss: 2.7608  L_pred:2.7602  L_time:0.0013    train.py:203
           Loss:2.7608                                                          
[06:53:14] Epoch 308  Loss: 2.7608  L_pred:2.7602  L_time:0.0013    train.py:203
           Loss:2.7608                                                          
[06:54:18] Epoch 309  Loss: 2.7609  L_pred:2.7603  L_time:0.0012    train.py:203
           Loss:2.7609                                                          
[06:55:23] Epoch 310  Loss: 2.7608  L_pred:2.7601  L_time:0.0013    train.py:203
           Loss:2.7608                                                          
[06:56:27] Epoch 311  Loss: 2.7606  L_pred:2.7599  L_time:0.0013    train.py:203
           Loss:2.7606                                                          
[06:57:31] Epoch 312  Loss: 2.7606  L_pred:2.7600  L_time:0.0012    train.py:203
           Loss:2.7606                                                          
[06:58:35] Epoch 313  Loss: 2.7606  L_pred:2.7600  L_time:0.0013    train.py:203
           Loss:2.7606                                                          
[06:59:39] Epoch 314  Loss: 2.7608  L_pred:2.7601  L_time:0.0012    train.py:203
           Loss:2.7608                                                          
[07:00:43] Epoch 315  Loss: 2.7604  L_pred:2.7598  L_time:0.0013    train.py:203
           Loss:2.7604                                                          
[07:01:47] Epoch 316  Loss: 2.7603  L_pred:2.7597  L_time:0.0012    train.py:203
           Loss:2.7603                                                          
[07:02:51] Epoch 317  Loss: 2.7602  L_pred:2.7596  L_time:0.0012    train.py:203
           Loss:2.7602                                                          
[07:03:55] Epoch 318  Loss: 2.7602  L_pred:2.7596  L_time:0.0013    train.py:203
           Loss:2.7602                                                          
[07:05:00] Epoch 319  Loss: 2.7603  L_pred:2.7597  L_time:0.0012    train.py:203
           Loss:2.7603                                                          
[07:06:03] Epoch 320  Loss: 2.7602  L_pred:2.7596  L_time:0.0013    train.py:203
           Loss:2.7602                                                          
[07:07:08] Epoch 321  Loss: 2.7599  L_pred:2.7592  L_time:0.0012    train.py:203
           Loss:2.7599                                                          
[07:08:12] Epoch 322  Loss: 2.7600  L_pred:2.7594  L_time:0.0012    train.py:203
           Loss:2.7600                                                          
[07:09:16] Epoch 323  Loss: 2.7601  L_pred:2.7595  L_time:0.0012    train.py:203
           Loss:2.7601                                                          
[07:10:20] Epoch 324  Loss: 2.7599  L_pred:2.7593  L_time:0.0012    train.py:203
           Loss:2.7599                                                          
[07:11:24] Epoch 325  Loss: 2.7598  L_pred:2.7592  L_time:0.0012    train.py:203
           Loss:2.7598                                                          
Eval Epoch 325 (team) MALE [0.3962603807449341, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.462978333234787, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597, 
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch325_male0_0.3963_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
[07:18:00] Epoch 326  Loss: 2.7598  L_pred:2.7592  L_time:0.0012    train.py:203
           Loss:2.7598                                                          
[07:19:04] Epoch 327  Loss: 2.7598  L_pred:2.7592  L_time:0.0012    train.py:203
           Loss:2.7598                                                          
[07:20:08] Epoch 328  Loss: 2.7597  L_pred:2.7591  L_time:0.0012    train.py:203
           Loss:2.7597                                                          
[07:21:12] Epoch 329  Loss: 2.7597  L_pred:2.7591  L_time:0.0012    train.py:203
           Loss:2.7597                                                          
[07:22:16] Epoch 330  Loss: 2.7595  L_pred:2.7589  L_time:0.0012    train.py:203
           Loss:2.7595                                                          
[07:23:21] Epoch 331  Loss: 2.7596  L_pred:2.7590  L_time:0.0012    train.py:203
           Loss:2.7596                                                          
[07:24:25] Epoch 332  Loss: 2.7595  L_pred:2.7589  L_time:0.0012    train.py:203
           Loss:2.7595                                                          
[07:25:29] Epoch 333  Loss: 2.7595  L_pred:2.7589  L_time:0.0012    train.py:203
           Loss:2.7595                                                          
[07:26:33] Epoch 334  Loss: 2.7596  L_pred:2.7590  L_time:0.0012    train.py:203
           Loss:2.7596                                                          
[07:27:38] Epoch 335  Loss: 2.7593  L_pred:2.7587  L_time:0.0012    train.py:203
           Loss:2.7593                                                          
[07:28:42] Epoch 336  Loss: 2.7592  L_pred:2.7586  L_time:0.0012    train.py:203
           Loss:2.7592                                                          
[07:29:46] Epoch 337  Loss: 2.7592  L_pred:2.7586  L_time:0.0012    train.py:203
           Loss:2.7592                                                          
[07:30:51] Epoch 338  Loss: 2.7594  L_pred:2.7588  L_time:0.0012    train.py:203
           Loss:2.7594                                                          
[07:31:56] Epoch 339  Loss: 2.7596  L_pred:2.7590  L_time:0.0012    train.py:203
           Loss:2.7596                                                          
[07:33:00] Epoch 340  Loss: 2.7592  L_pred:2.7586  L_time:0.0012    train.py:203
           Loss:2.7592                                                          
[07:34:04] Epoch 341  Loss: 2.7591  L_pred:2.7585  L_time:0.0012    train.py:203
           Loss:2.7591                                                          
[07:35:09] Epoch 342  Loss: 2.7589  L_pred:2.7583  L_time:0.0012    train.py:203
           Loss:2.7589                                                          
[07:36:13] Epoch 343  Loss: 2.7592  L_pred:2.7586  L_time:0.0012    train.py:203
           Loss:2.7592                                                          
[07:37:18] Epoch 344  Loss: 2.7593  L_pred:2.7587  L_time:0.0012    train.py:203
           Loss:2.7593                                                          
[07:38:22] Epoch 345  Loss: 2.7590  L_pred:2.7584  L_time:0.0012    train.py:203
           Loss:2.7590                                                          
[07:39:26] Epoch 346  Loss: 2.7588  L_pred:2.7582  L_time:0.0012    train.py:203
           Loss:2.7588                                                          
[07:40:30] Epoch 347  Loss: 2.7589  L_pred:2.7583  L_time:0.0012    train.py:203
           Loss:2.7589                                                          
[07:41:34] Epoch 348  Loss: 2.7589  L_pred:2.7582  L_time:0.0012    train.py:203
           Loss:2.7589                                                          
[07:42:39] Epoch 349  Loss: 2.7588  L_pred:2.7582  L_time:0.0012    train.py:203
           Loss:2.7588                                                          
[07:43:43] Epoch 350  Loss: 2.7586  L_pred:2.7580  L_time:0.0012    train.py:203
           Loss:2.7586                                                          
Eval Epoch 350 (team) MALE [0.3834146559238434, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.4500613212585449, 1.1996724605560303, 1.3402553796768188, 1.3455837965011597,
1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch350_male0_0.3834_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
[07:50:27] Epoch 351  Loss: 2.7585  L_pred:2.7580  L_time:0.0012    train.py:203
           Loss:2.7585                                                          
[07:51:32] Epoch 352  Loss: 2.7585  L_pred:2.7579  L_time:0.0012    train.py:203
           Loss:2.7585                                                          
[07:52:36] Epoch 353  Loss: 2.7585  L_pred:2.7579  L_time:0.0012    train.py:203
           Loss:2.7585                                                          
[07:53:41] Epoch 354  Loss: 2.7585  L_pred:2.7579  L_time:0.0012    train.py:203
           Loss:2.7585                                                          
[07:54:45] Epoch 355  Loss: 2.7585  L_pred:2.7579  L_time:0.0012    train.py:203
           Loss:2.7585                                                          
[07:55:49] Epoch 356  Loss: 2.7584  L_pred:2.7578  L_time:0.0011    train.py:203
           Loss:2.7584                                                          
[07:56:54] Epoch 357  Loss: 2.7583  L_pred:2.7577  L_time:0.0011    train.py:203
           Loss:2.7583                                                          
[07:57:58] Epoch 358  Loss: 2.7583  L_pred:2.7577  L_time:0.0011    train.py:203
           Loss:2.7583                                                          
[07:59:02] Epoch 359  Loss: 2.7583  L_pred:2.7577  L_time:0.0012    train.py:203
           Loss:2.7583                                                          
[08:00:06] Epoch 360  Loss: 2.7582  L_pred:2.7576  L_time:0.0012    train.py:203
           Loss:2.7582                                                          
[08:01:10] Epoch 361  Loss: 2.7582  L_pred:2.7576  L_time:0.0012    train.py:203
           Loss:2.7582                                                          
[08:02:14] Epoch 362  Loss: 2.7582  L_pred:2.7576  L_time:0.0012    train.py:203
           Loss:2.7582                                                          
[08:03:19] Epoch 363  Loss: 2.7581  L_pred:2.7575  L_time:0.0011    train.py:203
           Loss:2.7581                                                          
[08:04:23] Epoch 364  Loss: 2.7580  L_pred:2.7575  L_time:0.0012    train.py:203
           Loss:2.7580                                                          
[08:05:27] Epoch 365  Loss: 2.7580  L_pred:2.7574  L_time:0.0011    train.py:203
           Loss:2.7580                                                          
[08:06:31] Epoch 366  Loss: 2.7580  L_pred:2.7574  L_time:0.0012    train.py:203
           Loss:2.7580                                                          
[08:07:35] Epoch 367  Loss: 2.7579  L_pred:2.7573  L_time:0.0011    train.py:203
           Loss:2.7579                                                          
[08:08:39] Epoch 368  Loss: 2.7578  L_pred:2.7573  L_time:0.0011    train.py:203
           Loss:2.7578                                                          
[08:09:43] Epoch 369  Loss: 2.7579  L_pred:2.7574  L_time:0.0012    train.py:203
           Loss:2.7579                                                          
[08:10:47] Epoch 370  Loss: 2.7578  L_pred:2.7572  L_time:0.0012    train.py:203
           Loss:2.7578                                                          
[08:11:52] Epoch 371  Loss: 2.7577  L_pred:2.7572  L_time:0.0011    train.py:203
           Loss:2.7577                                                          
[08:12:56] Epoch 372  Loss: 2.7577  L_pred:2.7571  L_time:0.0011    train.py:203
           Loss:2.7577                                                          
[08:14:00] Epoch 373  Loss: 2.7577  L_pred:2.7571  L_time:0.0011    train.py:203
           Loss:2.7577                                                          
[08:15:05] Epoch 374  Loss: 2.7576  L_pred:2.7570  L_time:0.0011    train.py:203
           Loss:2.7576                                                          
[08:16:09] Epoch 375  Loss: 2.7576  L_pred:2.7570  L_time:0.0012    train.py:203
           Loss:2.7576                                                          
Eval Epoch 375 (team) MALE [0.37799787521362305, 0.8730535507202148, 
1.012279987335205, 1.0098925828933716, 0.9849193692207336]  RMSLE 
[0.44462862610816956, 1.1996724605560303, 1.3402553796768188, 
1.3455837965011597, 1.3298697471618652]
Evaluated model checkpoint saved: 
runs/20250508_001635_team/evaluated_model_epoch375_male0_0.3780_male1_0.8731_mal
e2_1.0123_male3_1.0099_male4_0.9849_team.pt
Removed old best model: 
runs/20250508_001635_team/best_model_epoch300_male0.3829_team.pt
New best model saved: 
runs/20250508_001635_team/best_model_epoch375_male0.3780_team.pt (MALE: 0.3780 
at epoch 375)
[08:22:53] Epoch 376  Loss: 2.7576  L_pred:2.7570  L_time:0.0011    train.py:203
           Loss:2.7576                                                          
[08:23:57] Epoch 377  Loss: 2.7576  L_pred:2.7570  L_time:0.0011    train.py:203
           Loss:2.7576                                                          
[08:25:01] Epoch 378  Loss: 2.7575  L_pred:2.7569  L_time:0.0011    train.py:203
           Loss:2.7575                                                          
[08:26:06] Epoch 379  Loss: 2.7574  L_pred:2.7568  L_time:0.0011    train.py:203
           Loss:2.7574                                                          
[08:27:10] Epoch 380  Loss: 2.7574  L_pred:2.7568  L_time:0.0011    train.py:203
           Loss:2.7574                                                          
[08:28:15] Epoch 381  Loss: 2.7575  L_pred:2.7570  L_time:0.0011    train.py:203
           Loss:2.7575                                                          
[08:29:19] Epoch 382  Loss: 2.7576  L_pred:2.7570  L_time:0.0012    train.py:203
           Loss:2.7576                                                          
[08:30:24] Epoch 383  Loss: 2.7575  L_pred:2.7569  L_time:0.0011    train.py:203
           Loss:2.7575                                                          
